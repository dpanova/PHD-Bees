

from BeeLitReview import BeeLitReview
review = BeeLitReview()
# review.researchgate_scraper()
review.scraped_data_enhancement()
review.encode_with_transformers()
review.calculate_similarity()
review.calculate_TSP()
#%%
results = review.dbscan_clusters(cosine_threshold = 0.5
                        ,year=2023
                        ,type = ['Article', 'Conference Paper']
                        ,read = 10
                        ,citation = 1)
#%%
#define a function for the table format table creation

def pd_to_tuple(df,col):
    pandas_table = pd.DataFrame(df[col].value_counts())

    pandas_table.reset_index(inplace=True)
    pandas_table[col] = pandas_table[col].astype('str')
    pandas_table['count'] = pandas_table['count'].astype('str')
    cols = tuple(pandas_table.columns)
    table_data = [tuple(x) for x in pandas_table.to_numpy()]
    table_data.insert(0,cols)
    table_data = tuple(table_data)
    return table_data

#%%




#%%
from fpdf import FPDF
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
from io import BytesIO
from nltk.corpus import stopwords
from wordcloud import WordCloud
from nltk.stem.snowball import SnowballStemmer
import nltk
# Generate the PDF report and save it
pdf = FPDF('P', 'mm', 'A4')

# page 1
# Report title, author, data of the report and disclaimer and dependant variable distribution
pdf.add_page()

pdf.set_margins(10, 10, 10)
pdf.set_font('Arial', 'B', 24)
pdf.cell(w = 180, h = 10, txt='Literature Review Automated Report', align= 'C')
pdf.ln(20)
pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt='Author: Denitsa Panova')
pdf.set_font('Arial', size= 10,style='I')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='Date: '+datetime.today().strftime('%Y-%m-%d') )
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='Disclaimer: The objective of this report is to present the outcomes generated by automated literature research.' )
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='A specialized interpretation is essential to derive accurate conclusions regarding correct articles to be reviewed.')

# Stats on how many articles have been investigated
pdf.ln(15)
pdf.set_font('Arial', size= 16, style='B')
pdf.cell(w = 40, h = 10, txt='Data Overview')
pdf.ln(10)
pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt='Query "%s" has been executed in researchgate.com and all available results are scraped - ' %review.query)
pdf.ln(5)
pdf.cell(w = 40, h = 10,txt = 'in total %s. Note, that in the context of this report, a result is a search result, it can be an article, presentation, etc.'%str(len(review.df)))
pdf.ln(10)
pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Time Distribution')
pdf.ln(10)
pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt="Firstly, we will investigate how the results's distribution over the years.")
pdf.ln(10)
pdf.set_font("Arial", size=10)

table_data = pd_to_tuple(review.df,'Year')

with pdf.table() as table:
    for data_row in table_data:
        row = table.row()
        for datum in data_row:
            row.cell(datum)
pdf.ln(10)
pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Word Distribution')
pdf.ln(5)
pdf.set_font('Arial', size= 10)
pdf.ln(10)
pdf.cell(w = 40, h = 10, txt='Then, we will investigate what is the average count of words per abstract. Here the goal is to see if the default transformer')

pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='model is still an adequate solution.The default model is all-mpnet-base-v2 and if the word count is above 384, it truncates')

pdf.ln(5)
pdf.cell(w = 40, h = 10, txt="the text and we wouldn't have full results in the encoding stage. We have %s of the results consenting the criteria." %str(round(sum(review.df['Abstract Count Words'] <= 384.0) / len(review.df),2)))
pdf.ln(10)


plt.figure()
review.df['Abstract Count Words'].hist()
# Converting Figure to an image:
img_buf = BytesIO()  # Create image object
plt.savefig(img_buf, dpi=100)  # Save the image
# pdf.image(img_buf, w=pdf.epw)  # Make the image full width
# pdf.image(img_buf, keep_aspect_ratio=True)
pdf.image(img_buf, x = 50, y = 200, w = 110, h = 0)

# img_buf.close()

#page 2

pdf.add_page()
pdf.set_margins(10, 10, 10)
pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Result type')
pdf.ln(10)
pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt='Investigate what results are extracted. This will help in the DBSCAN clustering choice for results. The default types for ')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='DBscan clustering are: Article and Conference Paper. ')

table_data_type = pd_to_tuple(review.df,'Text Type')
pdf.ln(10)
with pdf.table() as table:
    for data_row in table_data_type:
        row = table.row()
        for datum in data_row:
            row.cell(datum)

pdf.ln(10)
pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Language')
pdf.ln(10)
pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt='Investigate what languages the results are. We are considering only English languages for the purposes of this research.')
pdf.ln(5)

pdf.cell(w = 40, h = 10, txt='After removing non-English results, we lose %s of the data.' %str(round(len(review.df_raw[review.df_raw['Language']!='en'])/len(review.df_raw),2)))


table_data_language = pd_to_tuple(review.df_raw,'Language')
pdf.ln(10)
with pdf.table() as table:
    for data_row in table_data_language:
        row = table.row()
        for datum in data_row:
            row.cell(datum)


pdf.ln(10)
pdf.set_font('Arial', size= 16, style='B')
pdf.cell(w = 40, h = 10, txt='Results of the Conducted Research')
pdf.ln(10)
pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Travelling Salesmen Problem (TSP)')
pdf.ln(10)
pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt='Each abstract is turned into an embedding, using the HuggingFace Transformer. The default one is all-mpnet-base-v2.')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='After we calculate similarity between each scraped result with each another one, we apply Travelling Salesmen Problem')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt= 'for finding the shortest path. The time for calculating it is %s ms.' %str((review.graph_end - review.graph_start) * 10 ** 3))
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt= 'Additionally, to optimize the performance of the algorithm, we created an artificial connection in the graph')





#page 3
pdf.add_page()

pdf.set_margins(10, 10, 10)
initial_point = str(results.loc[0,'index'])

pdf.set_font('Arial', size= 10)
pdf.cell(w = 40, h = 10, txt= 'between the query %s and the most similar result in terms of cosine similarity between ' %review.query)
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt= 'embeddings. Then we artificially assigned similarity 0 to impose the algorithm to start from there. The result index is %s' %initial_point)

pdf.ln(10)

pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Similarity Difference')
pdf.set_font('Arial', size= 10)
pdf.ln(10)
pdf.cell(w = 40, h = 10, txt='In order to investigate what is the added value of TSP, we check what is the average similarity measure ')
pdf.ln(5)

path = pd.read_csv('path.csv')
#select only the inspected articles
path = path[:50]
#create tuples based on the path pairs
path_similarity = pd.DataFrame()
path_similarity['pair0'] = list(path.iloc[:-1,0])
path_similarity['pair1'] = list(path.iloc[1:,0])
path_similarity = path_similarity.merge(review.similarity_df, how='inner', on = ['pair0','pair1'])
# path_similarity['cos'].mean() #0.303075465137538101

#let us do the same for the original research order
original_similarity = pd.DataFrame()
original_similarity['pair0'] = list(range(0,49))
original_similarity['pair1'] = list(range(1,50))
original_similarity = original_similarity.merge(review.similarity_df, how='inner', on = ['pair0','pair1'])
# original_similarity['cos'].mean() #0.36051554165103217

pdf.cell(w = 40, h = 10, txt='in the first 50 TSP-suggested articles and the first 50 ResearchGate articles. TSP is %s and ResearchGate is %s.' %(str(round(path_similarity['cos'].mean(),2)),str(round(original_similarity['cos'].mean() ,2))))
pdf.ln(10)

pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt='Time Saved')
pdf.set_font('Arial', size= 10)
pdf.ln(10)

all_not_read_indix = list(set(list(review.df['index'])).difference(set(list(path.loc[:, '0']))))
all_not_read_indix_max = [x for x in all_not_read_indix if x < max(list(list(path.loc[:, '0'])))]
# the cost in terms of time
cost_time = str(round(review.df.loc[all_not_read_indix_max, 'Abstract Count Words'].sum() / 150 / 60,2))  # 12 hours

pdf.cell(w = 40, h = 10, txt='Now we will calculate how much time it is saved by following the 50 articles suggested by TSP. The goal is to ')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='understand how much time is saved and at the same time wider knowledge on the topic is acquired. The metric')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='encompasses the number of abstracts one should go over to read the same TSP-suggested results. According')
pdf.ln(5)

pdf.cell(w = 40, h = 10, txt='to a research, one can read around 150 words per minute. Therefore, the saved time by going with the TSP')
pdf.ln(5)
pdf.cell(w = 40, h = 10, txt='suggestion is %s hours.' %cost_time)



pdf.ln(10)

pdf.set_font('Arial', size= 12, style='B')
pdf.cell(w = 40, h = 10, txt= 'TSP Results of interest')
pdf.set_font('Arial', size= 10)
pdf.ln(10)
pdf.cell(w = 40, h = 10, txt='Let us look into the wordcloud of the TSP-results abstracts to see what those are in one look.')
pdf.ln(5)

to_review_df = pd.read_excel('to_review.xlsx')

words = nltk.word_tokenize(' . '.join(list(to_review_df['Title'])))

stop_words = set(stopwords.words('english'))

stemmer = SnowballStemmer("english")
words = [stemmer.stem(word.lower())
         for word in words
         if word.isalnum() and word.lower() not in stop_words]
word_freq = {}
for word in words:
    if word in word_freq:
        word_freq[word] += 1
    else:
        word_freq[word] = 1
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

plt.figure(figsize=(10, 5))  # width and height
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

img_buf = BytesIO()  # Create image object
plt.savefig(img_buf, dpi=100)  # Save the image
pdf.image(img_buf, x = 0, y = 100, w = 200, h = 0)

pdf.ln(100)


pdf.cell(w = 40, h = 10, txt='The next step is to look into the TSP results and identify those which are of interest for the research.')

pdf.ln(5)

pdf.cell(w = 40, h = 10, txt='We have identified those:')

pdf.ln()

for idx,row in to_review_df[to_review_df['Choose']].iterrows():
    pdf.cell(w=40, h=10, txt='Title: ' + row['Title'])
    pdf.ln(5)


# pdf.ln(10)
# pdf.set_font('Arial', size= 12, style='B')
# pdf.cell(w = 40, h = 10, txt= 'Clustering of the results')
# pdf.set_font('Arial', size= 10)
# pdf.ln(10)
# pdf.cell(w = 40, h = 10, txt='After having the suggested new reading route, the next step is to conduct DBSCAN clustering. The goal is find the simila')
# pdf.ln(5)
# pdf.cell(w = 40, h = 10, txt='After having the suggested new reading route, the next step is to conduct DBSCAN clustering.')
# pdf.ln(5)




img_buf.close()
pdf.output('lit_report.pdf', 'F')




#%%
# Dependant variable analysis
pdf.ln(15)
pdf.set_font('Arial', size= 16, style='B')
pdf.cell(w = 40, h = 10, txt='Dependant variable distribution')
#df = wine.data

pdf.ln(10)


pdf.set_font('Arial', size= 12)
pdf.cell(w = 40, h = 10, txt='Original Variable')
pdf.ln(5)
pdf.image('dependant_original.png', x = 10, y = 90, w = 130, h = 0)

pdf.ln(85)
pdf.cell(w = 40, h = 10, txt='Created Variable')
pdf.image('dependant_created.png', x = 10, y = 180, w = 130, h = 0)

# add footer
pdf.set_font('Arial', 'I', 8)
pdf.set_y(260)
pdf.cell(0, 10, 'Page ' + str(pdf.page_no()), 0, 0, 'C')


#%%


#what if we choose the starting point based on


#%%
# check how the dbscan will work

review.dbscan_clusters()

#%%
#identify the cost to read until those

#create a word cloud
to_review_df = pd.read_excel('to_review.xlsx')
# add the cost and what is the average cosine similarity


path = pd.read_csv('path.csv')
#select only the inspected articles
path = path[:100]
#create tuples based on the path pairs
path_similarity = pd.DataFrame()
path_similarity['pair0'] = list(path.iloc[:-1,0])
path_similarity['pair1'] = list(path.iloc[1:,0])
path_similarity = path_similarity.merge(review.similarity_df, how='inner', on = ['pair0','pair1'])
path_similarity['cos'].mean() #0.303075465137538101

#let us do the same for the original research order
original_similarity = pd.DataFrame()
original_similarity['pair0'] = list(range(0,99))
original_similarity['pair1'] = list(range(1,100))
original_similarity = original_similarity.merge(review.similarity_df, how='inner', on = ['pair0','pair1'])
original_similarity['cos'].mean() #0.36051554165103217
#%%
#cost to read
#get the difference between what has been read in order to get to the same lit research
all_not_read_indix = list(set(list(review.df['index'])).difference(set(list(path.loc[:,'0']))))
all_not_read_indix_max = [x for x in all_not_read_indix if x< max(list(list(path.loc[:,'0'])))]
#the cost in terms of time
review.df.loc[all_not_read_indix_max,'Abstract Count Words'].sum()/150/60 #12 hours



#%%
# #%%
# import pandas as pd
# to_review_df = pd.read_excel('to_review.xlsx')
#
# index_chosen = list(to_review_df[to_review_df['Choose']]['index'])
# year = 2023
# type = ['Article', 'Conference Paper']
# read = 10
# citation = 1
# filter_index = list(review.df[(review.df['Year']>=year) & (review.df['Text Type'].isin(type)) & (review.df['Citations']>=citation) & (review.df['Reads'] >=read)  ]['index'])
#
# all_index = index_chosen+filter_index
# review.embeddings
#
#
# to_cluster_embeddings = [review.embeddings[x] for x in all_index]
#
#
# #%%
#
# # def exclude_tuple(element_list,thetuple):
# #     """Return the tuple if none of the elements are present"""
# #     exclude = True
# #     for element in element_list:
# #         if element in thetuple:
# #             exclude = False
# #     if exclude:
# #         return thetuple
# #
# # def include_tuple(element_list,thetuple):
# #     """Return the tuple if at least one of the elements is present"""
# #     exclude = False
# #     for element in element_list:
# #         if element in thetuple:
# #             exclude = True
# #     return exclude
# #
# #
# # index_exclude_tuple = list(itertools.pairwise(to_review_df['index']))
# #
# # index_exclude_tuple_updated  = [x for x in index_exclude_tuple if exclude_tuple(index_chosen,x) is not None]
# #
# # cluster_similarity_df = review.similarity_df
# # #since it is a tuple, we need to take into account that pairs are interchangable
# # cluster_similarity_df['key'] = list(zip(cluster_similarity_df['pair0'], cluster_similarity_df['pair1']))
# # cluster_similarity_df['key1'] = list(zip(cluster_similarity_df['pair1'], cluster_similarity_df['pair0']))
# # cluster_similarity_df = cluster_similarity_df[~cluster_similarity_df['key'].isin(index_exclude_tuple_updated) & ~cluster_similarity_df['key1'].isin(index_exclude_tuple_updated) ]
# #
# #
# #
# # #get the cluster similarity of those indices under inspection
# # cluster_similarity_df['Choose'] = cluster_similarity_df['key'].apply(lambda x : include_tuple(index_chosen,x) )
# # cluster_similarity_df = cluster_similarity_df[cluster_similarity_df['Choose']]
# #
# # #get the indices of the articles we want to look at
# # fileter_index = review.df[(review.df['Year']>=year) & (review.df['Text Type'].isin(type)) & (review.df['Citations']>=citation) & (review.df['Reads'] >=read)  ]['index']
# # cluster_similarity_df['Choose Filter'] = cluster_similarity_df['key'].apply(lambda x : include_tuple(fileter_index,x) )
# # cluster_similarity_df = cluster_similarity_df[cluster_similarity_df['Choose Filter']]
#
# #the only thing left is the clustering
#
#
#
# #%%
# from sklearn.cluster import DBSCAN
# from numpy import arange
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import silhouette_score,make_scorer
# import numpy as np
# import scipy as sp
# def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):
#     # Result is noise by default
#     y_new = np.ones(shape=len(X_new), dtype=int)*-1
#
#     # Iterate all input samples for a label
#     for j, x_new in enumerate(X_new):
#         # Find a core sample closer than EPS
#         for i, x_core in enumerate(dbscan_model.components_):
#             if metric(x_new, x_core) < dbscan_model.eps:
#                 # Assign label of x_core to x_new
#                 y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]
#                 break
#
#     return y_new
#
# def my_custom_function(model, X, y=None):
#     # for models that implement it, e.g. KMeans, could use `predict` instead
#     preds = dbscan_predict(model, X)
#     return silhouette_score(X, preds) if len(set(preds)) > 1 else float('nan')
#
#
# # we can optimize for eps and min_samples
# param_grid = {
# 'eps': arange(0.1,0.5,0.001),
# 'min_samples': range(3,15,1)
# }
#
# dbscan = DBSCAN(metric = 'cosine')
# grid_search = GridSearchCV(dbscan, param_grid,scoring=my_custom_function)
# grid_search.fit(to_cluster_embeddings)
#
# best_eps = grid_search.best_params_['eps']
# best_min_samples = grid_search.best_params_['min_samples']
#
# dbscan_best = DBSCAN(eps=best_eps, min_samples=best_min_samples, metric='cosine')
# labels = dbscan_best.fit_predict(to_cluster_embeddings) # cluster data
#
# results = pd.DataFrame()
# results['cluster'] = labels
# results['index']=all_index
#
# results = results.merge(review.df, how='left', on='index')
#
#
# #%%
#
# def include_tuple(element_list,row, t_limit=0.3):
#     """Return the tuple if at least one of the elements is present"""
#     exclude = False
#     for element in element_list:
#         if (element in row['key']) & (row['cos']>=t_limit):
#             exclude = True
#     return exclude
#
# review.similarity_df['Under Inspection'] = False
# review.similarity_df
# for index, row in review.similarity_df.iterrows():
#     review.similarity_df.loc[index,'Under Inspection']= include_tuple(index_chosen,row,t_limit=0.3)
#
# # 1. create pairs based on the to_review_df index
# # 2. remove it from the similarity df
# # 3. extract only the chosen articles
# # 4. create filters for the date, citations and reads and filter based on those the similarity df
# # 5. cluastering? heirarchical? DBSCAN?
#%%

